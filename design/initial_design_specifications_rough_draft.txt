Summary Document: Custom Data Parsing and Transformation Engine
Overview
This document outlines the design and functionality of a custom data parsing and transformation engine inspired by Splunk but tailored for handling SQLite databases with Python3 on the backend. The engine is designed to aggregate data from one or more SQLite databases (referred to as indexes), apply various transformations, and output the processed data. It allows for complex queries involving data retrieval, transformations, joins, field operations, and formatting.
Core Features
Data Retrieval: The engine supports retrieving data from specified SQLite databases and their tables and returning a single, main, aggregated pandas df. Users can specify one or more databases as indexes and any number of tables (all by default) within those databases as sources.
Example of 2 indexes: `index=db0 index=db1`
Example of 1 index: `index=db0`
Example of  1 index and 1 source (a.k.a table): `index=finances source=2024_purchases`
Field Operations: Users can specify fields to include or exclude from the output. Multiple | fields operations can be applied to refine the selection iteratively.
Example of including fields: `| fields field0 field1`
Example of removing fields: `| fields - field2 field3`  <— Note that this includes ALL OTHER fields
Data Transformation: The engine mimics the | stats command for basic statistical operations explained and exemplified throughout this specification document and supports | eval functions for creating or modifying fields based on expressions, including arithmetic operations, string manipulations, and regex applications.
Example of grouping by a single field: `| stats count as event_count by field0` —> This should provide a new field called “event_count” which provides a count of appearances of “field0”. This should produce a single table with 2 fields: field0 and event_count
Example of grouping by a single field and aggregating all other field values into “multivalued” fields: `| stats values(*) count as event_count by field0` —> This basically should return a single, multivalued field for all fields and their values found present and NOT NULL AND NOT EMPTY for a given (aggregated) main pandas df. Fields that have null or empty values throughout, should not be included in the main aggregated pandas df returned from the query.
Example of grouping by a multiple fields and aggregating all other field values into “multivalued” fields: `| stats values(*) count as event_count by field0` —> This basically should return a single, multivalued field for all fields values found present and NOT NULL OR EMPTY for a given (aggregated) main pandas df. Fields that have null or empty values throughout, should not be included in the main aggregated pandas df returned.
Example of using | eval to answer adjust either an existing field, or create a new field: `| eval existing_field=some_other_field` —> This basically sets an existing field “existing_field” equal to another field’s “some_other_field” VALUE.
Join Operations: It supports joining data from different sources (tables) within the same or different indexes (databases), similar to Splunk's | join command. However, this implementation of the join command should basically ONLY provide for type=left join type and only on a single field that exists in the current main pandas df AND the target search’s aggregated main returned dataframe (df). 
Example of a join with the initial search included. (Note that the numbers appended to the variables in this example are just to make the point that great variety should be able to be achieved with combinations of these seemingly simple rules described in this specification document) :
```
index=db0 index=db1 source=table1,”some table4”,table14,table54 WHERE field0=value1 OR field1=value45
| join field92 [index=db18 source=table_34 WHERE field23=value22 | fields field92, field48]
```
^ Note that in the previous join example that it is expected that ALL NON-Null, NON-EMPTY fields (that is, fields that have at least 1 non-null, non-empty value in SOME record within the field’s column) are returned, not just specified in the WHERE clause. Also note the use of OR. Our functionality should also include AND functionality as well.
Another Join Example, with different structure that is also valid:
```
index=db0 WHERE “string_to_search”
| join field22 [index=db23 source=table2 WHERE “string_to_search”]
```
Note here how the “string_to_search” has no key set equal to it. This is because this is intended to instruct the engine to search ALL fields and their respective values for the case-insensitive existence of a provided string and return ALL matching records and non-null, non-empty fields. 
Grouping, Aggregation, and Sorting: Functionality to group data by specified fields and perform aggregations on grouped data. These are considered TRANSFORMING commands as they TRANSFORM the various aggregations currently present in the main pandas df, basically making a new, pivoted (if you will) pandas df.
Example of grouping and aggregating (aka TRANSFORMING) the current main table (aka pandas df):
```
index=db0 WHERE “string_to_search”
| stats values(*) count as event_count by field34, field53
```
^ This example basically first pulls all records and all non-null, non-empty fields and returns a single, main pandas dataframe (df) containing that information. Then, the complex | stats … call initiates the TRANSFORMing action by grouping all other fields (NOT field34 or field53) available in the main pandas df as multivalue fields, all grouped by the concatenation of unique values coming from the aggregation of field34 and field53. Also notice where a rename of the field can optionally happen within the stats call by utilizing the “as” clause after an operation. 
Example with sort:
```
index=db34
| stats values(name) as name, count as event_count by city, ‘Birth Year’
| sort - 100 event_count
```
^ Here we introduce the sort function that sorts the “event_count” field in descending order “-“, returning only the top “100” records after ordering is complete. If a number is not provided here, then ALL should be sorted and returned as the number part of the function is optional. If a “-“ for descending nor a “+” for ascending is provided, then a “+” ascending directive is the default as this parameter is optional as well. Additionally, notice how ‘Birth Year’ is surrounded by single quotes “‘“ and this is to denote that this is a variable name and not a string as strings are surrounded by double quotes.
Final Data Formatting: Allows for regex-based field value extractions and regex-based filtering, explicit field renaming, creation of new fields, perform basic calculations against fields or groups of fields, all enabling users to format the output data as needed.
Example regex based extraction (rex):
```
# Example usage of comments. This will be ignored by the engine.
index=db0 index=db3 source=table5*
| stats values(*) count as event_count by event_detailed_description
| rex field=event_detailed_description “https:\/\/site\.com(?<target_uri>\S+)”
| fields target_uri event_count event_detailed_description
```
^ Important Points: The | rex call MUST have a field=<target_field> and <target_field> must exist. Also, the provided regex should be passed back to the python3 engine and run via a similar backend function to extract any values found from the target regex and place them into a new field named after the capture group name, in this case it’s “target_uri”. Special note: Also, notice how I add a star after the table name, indicating that I want to include any table name that STARTS WITH “table5”, which could and should include table53 and table5434383784, etc, assuming those are found to exist within the db’s folder. 
```
# Example usage of regex to limit search results:
index=“credit cards.db”
| regex overdue!=“(?si)yes|for\ssure|help”
```
^ Here I create a query where I only want to see the good stuff, and not the bad reports on my credit cards. Notice that I can use = or != to either include or NOT include results respectively.
```
# Example usages of eval for new field creation:
# Example eval with sum
| eval total=income+passive_income  # Both income and passive_income must be number types.
# Example eval with subtraction
| eval total=income-total_costs  # Again for this and all following, they must all be number types.
# Example eval with divide
| eval cost_per_item=package_cost/item_count
# Example eval with divide and round
| eval cost_per_item=round(package_cost/item_count, 2)  # Example of rounding 2 places past the decimal.
# Example eval with multiply:
| eval profit_per_package=round(sale_profit*packages_sold, 2)  
# Example Complex, following order of operation of math as well as parenthesis first:
| eval ‘Total Sales’=round((sale_profit*packages_sold)/employee_count+100, 2)
# ^ The order should be (sale_profit*packages_sold), then that divided by employee_count, then that + 100, then that rounded to two decimal places and returned as a number in a new field/column named “total_sales”. Here again, I exemplify how a variable declaration for variable names that contain spaces must look (i.e. THEY MUST USE SINGLE “‘“ QUOTES.)
# Example String concatenation by simply specifying field names:
| eval new_field=concat(field0,’Some Field w spaces’,numeric_field,”some string etc”)
^ Notice here how numeric fields, string fields, and explicit strings can be concatenated in any order.
Query Structure
Index Specification: Users start queries with index=<database_name> to specify the target database(s). The wildcard * can be used to target all available databases. It can also be used to target partial index names, such as `index=db5*` which should include any database whose name STARTSWITH the case-insensitive provided string, here in this case is found to be “db5” such as “db544” or “db556” or even just “db5” itself as the wildcard should indicate 0 or more.
Source Specification: Following the index, users can specify tables within the databases as sources using source=<table_name>. The source field can use wildcards just as index can and in the same consistent fashions as described in the Index Specification section.
Where Clause: Optional filtering based on field values can be applied directly after specifying the index and source. This can always be found to the right of a capitalized WHERE clause in the initial line. WHERE clause values can be single strings “ExAmPle StrinG” which will be case-insensitively checked against all fields in the target database/source query, returning only the records and fields that contain non-null, non-empty, matching values. Additionally, WHERE clause values can have key=value pairs, each separated by an “=“ equal sign. The values provided in key-value pairs to the WHERE clause, however, will only check the case-insensitive values against the field indicated by the key name. Therefore, keys provided here must case-insensitively match the target field values ONLY within the field specified and NOT for every field value regardless such as how the “single strings” example had mentioned.
Field Inclusions/Exclusions: Users can specify which fields to include or exclude from the result set using | fields <field_list> or | fields - <field_list>. This operation can be done multiple times anywhere in the query as long as it’s AFTER the index, source, and WHERE clause calls. 
Eval Operations: Multiple, optional| eval operations allow users to dynamically create or modify fields based on specified expressions.
Example of creating a new field with eval: `| eval full_name=concat(last_name, first_name, “,”)`
Implementation Details
Parsing and Processing: The engine includes a sophisticated query parser to decompose the user's query into actionable components, handling multiple specifications for field operations and eval transformations.
Data Aggregation and Transformation: It utilizes pandas DataFrames for aggregating and manipulating data, ensuring efficient processing and flexible data transformation capabilities.
Safety and Robustness: The implementation emphasizes safe evaluation of expressions and robust error handling to manage complex queries and edge cases effectively.
Query (Job) Logging & Caching

Each query executed should have verbose logging that captures backend operations that occur for each query execution (a.k.a. “jobs”) via the python3 logging library and this information should be recorded upon each job completion with logging events saved to a flat file, each named after the epoch timestamp of the time the query was executed by the engine. The results of each query should be saved to a separate file in the same folder location as the logs (referred to from here forward as the JOBS folder), and should save the main pandas data frame for each job for caching/performance purposes, each also with the same naming schema aforementioned for log files for the same job. When a query is run, the query should be checked to see if said exact query already exists in the collection of cached results found in the JOBS folder and return the already formatted results, if found. Otherwise, the query should execute and then save the results to one new file and the logging to another new file within the JOBS folder and THEN ALSO DELETE THE OLDEST JOB RESULTS FILE IF AND ONLY IF THE COUNT OF JOBS PRESENT IN THE JOBS FOLDER EXCEEDS A GLOBAL COUNT THAT DEFAULTS TO 50.

Usage Example via CLI
```
./data_engine.py 'index=mydatabase | fields name, age | eval new_field=name_age | fields - temporary_field'
```
^ This command retrieves data from mydatabase, includes only name and age fields, creates a new field new_field by combining name and age, and excludes temporary_field from the output.
Future Extensions
Introduce optimization techniques for processing large datasets and improving performance.
Introduce more TRANSFORM functions to the extensible engine that can be called with a leading pipe anytime after the initial index/source & WHERE clauses (ie.first line)

Design Principles
Things should be as extensible as possible, specifically with the transforming functions as I’m sure I’ll want to add more as time goes by and it would be nice if they could be added simply by creating individual python3 py files, each containing python3 routines that transform input to output, passing the resulting, transformed pandas data frame back to the main engine. 
DOC strings for functions or elsewhere should be VERY concise, one-two lines tops.
Robustness, Efficiency, and Extensibility are absolutely key to this engine’s success. Please ensure this in all facets of the design.


Conclusion
This custom data parsing and transformation engine offers a flexible and powerful tool for data analysis and manipulation, drawing inspiration from Splunk's capabilities but focused on SQLite databases. It provides a foundation for further development and customization to meet specific data processing needs.
Approach
Given the complexity of this project, we should likely iterate through this after formulating a well-thought out plan and collective architecture based on all of my aforementioned specifications. Please deeply review and deeply understand the full feature set and engine described here before proceeding to put together a plan on how to approach building this out in a robust, efficient, and concise fashion, piece-by-piece.


